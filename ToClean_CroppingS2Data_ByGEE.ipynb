{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToClean_CroppingS2Data_ByGEE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNlN68gr170VTuNbB75wvvq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nunocesarsa/Examples/blob/main/ToClean_CroppingS2Data_ByGEE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fxktMTjSPva"
      },
      "source": [
        "Connecting to Google Earth Engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7ug-cJ6Lf10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "4d9e331c-89fa-496d-b7fa-337569a576da"
      },
      "source": [
        "#importing Earth Engine package\n",
        "import ee\n",
        "\n",
        "#then we need to authenticate\n",
        "# Trigger the authentication flow.\n",
        "ee.Authenticate()\n",
        "\n",
        "# Initialize the library.\n",
        "ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=Fz-JkxhRqoYb7H-J_oNtkPJFn3Y8BX_TY0F6seMWIeU&code_challenge_method=S256\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below. \n",
            "Enter verification code: 4/0AGD5qA36AfjbEgfyOTqyWztQ8M7XQHGrCMa5ip5KXrubNPZH97GwK8\n",
            "\n",
            "Successfully saved authorization token.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzX4L4m2SU-c"
      },
      "source": [
        "Mounting google drive to save the outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX-WkuCrSYVE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8ec8f372-e1ee-43d0-ffd6-f196808911b8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY2BNK4QSZMG"
      },
      "source": [
        "Installing needed packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSyo88IoScGU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "cdee7e58-6198-4201-a325-b05f0bc7eab7"
      },
      "source": [
        "!pip install pyrsgis\n",
        "!pip install rasterio\n",
        "!pip install pyproj\n",
        "!pip install geopandas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyrsgis\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/80/a74e3d968e2b74450d7de85dbd13785c73e8580a956fa56d14973266a9bf/pyrsgis-0.3.1-py3-none-any.whl\n",
            "Installing collected packages: pyrsgis\n",
            "Successfully installed pyrsgis-0.3.1\n",
            "Collecting rasterio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/76/7487c1eefeb8f3a0a60b37d74702c63c1b4ea701e8ce16bb7283b7f20a1f/rasterio-1.1.4-cp36-cp36m-manylinux1_x86_64.whl (18.2MB)\n",
            "\u001b[K     |████████████████████████████████| 18.2MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from rasterio) (19.3.0)\n",
            "Collecting click-plugins\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl\n",
            "Collecting snuggs>=1.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/0e/d27d6e806d6c0d1a2cfdc5d1f088e42339a0a54a09c3343f7f81ec8947ea/snuggs-1.4.7-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.18.4)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from rasterio) (7.1.2)\n",
            "Collecting affine\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/a6/1a39a1ede71210e3ddaf623982b06ecfc5c5c03741ae659073159184cd3e/affine-2.3.0-py2.py3-none-any.whl\n",
            "Collecting cligj>=0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/be/30a58b4b0733850280d01f8bd132591b4668ed5c7046761098d665ac2174/cligj-0.5.0-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from snuggs>=1.4.1->rasterio) (2.4.7)\n",
            "Installing collected packages: click-plugins, snuggs, affine, cligj, rasterio\n",
            "Successfully installed affine-2.3.0 click-plugins-1.1.1 cligj-0.5.0 rasterio-1.1.4 snuggs-1.4.7\n",
            "Collecting pyproj\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c3/071e080230ac4b6c64f1a2e2f9161c9737a2bc7b683d2c90b024825000c0/pyproj-2.6.1.post1-cp36-cp36m-manylinux2010_x86_64.whl (10.9MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9MB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: pyproj\n",
            "Successfully installed pyproj-2.6.1.post1\n",
            "Collecting geopandas\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/c5/3cf9cdc39a6f2552922f79915f36b45a95b71fd343cfc51170a5b6ddb6e8/geopandas-0.7.0-py2.py3-none-any.whl (928kB)\n",
            "\u001b[K     |████████████████████████████████| 931kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyproj>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from geopandas) (2.6.1.post1)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.0.3)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.7.0)\n",
            "Collecting fiona\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/20/4e63bc5c6e62df889297b382c3ccd4a7a488b00946aaaf81a118158c6f09/Fiona-1.8.13.post1-cp36-cp36m-manylinux1_x86_64.whl (14.7MB)\n",
            "\u001b[K     |████████████████████████████████| 14.7MB 313kB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (1.18.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (2018.9)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (19.3.0)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (1.1.1)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (0.5.0)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (7.1.2)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (1.12.0)\n",
            "Installing collected packages: munch, fiona, geopandas\n",
            "Successfully installed fiona-1.8.13.post1 geopandas-0.7.0 munch-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP63F_NRSd_5"
      },
      "source": [
        "Importing packages that might (or not) be useful later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVLruRtjShN4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4917db8e-180d-4863-f7cd-743a310c261b"
      },
      "source": [
        "#General purpose: \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "from pyrsgis import raster\n",
        "\n",
        "import rasterio\n",
        "import rasterio.plot\n",
        "import pyproj\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import geopandas as gpd\n",
        "\n",
        "\n",
        "from pyrsgis.convert import changeDimension"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning! matplotlib_scalebar library not found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "213z21SYSivk"
      },
      "source": [
        "Exploring loaded shapefiles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ske8R6SiofAw"
      },
      "source": [
        "redouan_shp = gpd.read_file(\"/content/drive/My Drive/Redouan/Area_MapsWGS84_CutBlocks.shp\")\n",
        "redouan_shp.plot()\n",
        "\n",
        "#and we can also plot each subsection individually\n",
        "for i in redouan_shp['id']:\n",
        "  area_sel = redouan_shp.loc[redouan_shp['id']==i]\n",
        "  area_sel.plot()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWIfAi1GJZFr"
      },
      "source": [
        "from datetime import datetime\n",
        "from pytz import timezone    \n",
        "\n",
        "#to have a sense of the time it takes\n",
        "berlin_time = timezone('Europe/Berlin')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnWcEbup7wgi"
      },
      "source": [
        "Core loop - Please load auxiliar functions at the bottom \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej8FnhWL70vG"
      },
      "source": [
        "#setting parameters for inversion\n",
        "n_traits= 4\n",
        "n_samples = 4000\n",
        "\n",
        "label_names = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "training_noise_level = .05\n",
        "\n",
        "number_epochs = 300\n",
        "\n",
        "#output folder\n",
        "path2outfld = '/content/drive/My Drive/Redouan'\n",
        "\n",
        "#auxiliar function\n",
        "def coords_from_gpd(poly):\n",
        "  return eval(poly.to_json())['features'][0]['geometry']['coordinates']\n",
        "\n",
        "for i in redouan_shp['id']:\n",
        "\n",
        "  #selects the ith shape block and creates a ee.shape out of it\n",
        "  area_sel = redouan_shp.loc[redouan_shp['id']==i]\n",
        "  #area_sel.plot()\n",
        "  area_ee = ee.Geometry.Polygon(coords_from_gpd(area_sel))\n",
        "\n",
        "\n",
        "  #fetches all available iamges in the area based on the shape and time\n",
        "  s2_collection = (ee.ImageCollection(\"COPERNICUS/S2_SR\").\n",
        "                   select(['B2','B3','B4',\n",
        "                           'B5','B6','B7',\n",
        "                           'B8A','B11','B12']).\n",
        "                   filter(ee.Filter.date('2019-06-01','2019-07-31')).\n",
        "                   filterBounds(area_ee).\n",
        "                   filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',10)))\n",
        "  count = s2_collection.size()\n",
        "  print('Nr of images in collection: ', str(count.getInfo())+'\\n')\n",
        "\n",
        "  k_set = 1\n",
        "  #here, it will download and retrieve the parameters using a multitask neural network\n",
        "  for j in range(count.getInfo()):\n",
        "    print('Downloading and retrieving parameters on area '+ str(j+1) + ' image ' + str(i+1)) + \" of \" + count.getInfo()\n",
        "\n",
        "\n",
        "    #fetching the image\n",
        "    mid_step = s2_collection.toList(count)\n",
        "    s2_img = ee.Image(mid_step.get(j))\n",
        "    s2_img_meta = s2_img.getInfo()\n",
        "\n",
        "    #and printing out the name to confirm i got the right thing\n",
        "    out_s2_name = s2_img_meta.get('properties',{}).get('PRODUCT_ID')[0:19]+\"AreaID\"+str(i) + \"Set_\"+ str(k_set)\n",
        "    print(\"Loaded:\" + s2_img_meta.get('properties',{}).get('PRODUCT_ID'))\n",
        "\n",
        "    ################################################# the bulk data downloading starts here  #################################################\n",
        "\n",
        "    #Creates the task for EE\n",
        "    task_config = {\n",
        "      'image': s2_img,\n",
        "      'fileFormat': 'GeoTIFF',\n",
        "      'folder': 'Redouan_out',\n",
        "      'fileNamePrefix': out_s2_name,\n",
        "      'description': \"clipped area\",\n",
        "      'scale':20,\n",
        "      'region':area_ee}\n",
        "    \n",
        "    #starts the task\n",
        "    print(\"Google Earth engine task started\")\n",
        "    task = ee.batch.Export.image.toDrive(**task_config )\n",
        "    task.start()\n",
        "\n",
        "    #creates a loop that is infinite until the its finished\n",
        "    flag = task.status()['state']\n",
        "    while flag != 'COMPLETED':\n",
        "      flag = task.status()['state']\n",
        "      #print(k,flag)\n",
        "      if flag == 'FAILED':\n",
        "\n",
        "        print(\"GEE data cropping failed - proceeding to next image\")\n",
        "        print(\"Failed: \"+ out_s2_name)\n",
        "        flag = 'COMPLETED' \n",
        "    print(\"Saved at: \" + \"/content/drive/My Drive/Redouan_out/\" + out_s2_name + \".tif\")\n",
        "\n",
        "\n",
        "     ################################################# the bulk model training starts here  #################################################\n",
        "\n",
        "    #selecting the sensor noise convolution matrix\n",
        "    sens_tag = out_s2_name[0:3]\n",
        "    if sens_tag == 'S2A':\n",
        "      filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "      print(\"Sensor found: S2A\")\n",
        "    if sens_tag == 'S2B':\n",
        "      filepath=\"/content/drive/My Drive/S2_Responses_S2B.csv\"\n",
        "      print(\"Sensor found: S2B\")\n",
        "\n",
        "    #LAtin hypercube sampling on a space of [0-1]\n",
        "    LHS_train = lhsmdu.createRandomStandardUniformMatrix(n_traits,n_samples)\n",
        "\n",
        "    #creating traits table\n",
        "    pd_trait = pd.DataFrame.transpose(pd.DataFrame(LHS_train))\n",
        "    pd_trait.columns = label_names\n",
        "    pd_trait[\"cab\"]=pd_trait[\"cab\"]*max_cab+min_cab\n",
        "    pd_trait[\"cw\"] =pd_trait[\"cw\"] *max_cw+min_cw\n",
        "    pd_trait[\"cm\"] =pd_trait[\"cm\"] *max_cm+min_cw\n",
        "    pd_trait[\"lai\"]=pd_trait[\"lai\"]*max_lai+min_lai    \n",
        "\n",
        "    #numpy format\n",
        "    np_trait = pd_trait.iloc[:,:].values\n",
        "\n",
        "    #fetching the metadata info on the sensor position\n",
        "    band_list = ['B2','B3','B4','B5','B6','B7','B8A','B11','B12']\n",
        "    inc_azi = 0 #starting value\n",
        "    inc_zen = 0\n",
        "    for k in band_list:\n",
        "      #print('MEAN_INCIDENCE_AZIMUTH_ANGLE_'+i)\n",
        "      #print(s2_img_meta.get('properties',{}).get('MEAN_INCIDENCE_AZIMUTH_ANGLE_'+i))\n",
        "      inc_azi = inc_azi + s2_img_meta.get('properties',{}).get('MEAN_INCIDENCE_AZIMUTH_ANGLE_'+k)/len(band_list)\n",
        "      inc_zen = inc_zen + s2_img_meta.get('properties',{}).get('MEAN_INCIDENCE_ZENITH_ANGLE_'+k)/len(band_list)\n",
        "\n",
        "    sol_azi = s2_img_meta.get('properties',{}).get('MEAN_SOLAR_AZIMUTH_ANGLE')\n",
        "    sol_zen = s2_img_meta.get('properties',{}).get('MEAN_SOLAR_ZENITH_ANGLE')\n",
        "\n",
        "    #and storing them on precise names as PROSAIL expects\n",
        "    tts = sol_zen\n",
        "    tto = inc_zen\n",
        "    phi = inc_azi-sol_azi  #according to the details of prosail\n",
        "    if phi < 0:\n",
        "      print(\"Negative phi:\" +str(phi) + \"inverting substraction\")\n",
        "      phi = sol_azi - inc_azi\n",
        "\n",
        "    print(\"Geometric parameters - tts: \"+ str(tts) +\" tto: \"+ str(tto) + \" phi: \" + str(phi))\n",
        "\n",
        "\n",
        "    #now we generate spectra using prosail \n",
        "    #the metadata of the input image MUST contribute here\n",
        "    print(\"Generating the spectra\")\n",
        "    np_spect = Gen_spectra_data(pd_trait,tts,tto,phi)[:,[1,2,3,4,5,6,8,11,12]]\n",
        "\n",
        "    #adding noise to the signal \n",
        "    print(\"Adding the noise of \",training_noise_level*100,\"% to training data\")\n",
        "    np_spect_comb = np.apply_along_axis(combined_noise,1,np_spect,sigma=training_noise_level) # 5% noise on the data, combined\n",
        "\n",
        "    #this probably would not be necessary but just in case i want to check the accuracy (basically sets aside 10% of the data)\n",
        "    index = list(range(len(np_spect)))\n",
        "    index10 = rdm.sample(index,math.ceil(len(index)*.1)) #randomly selects 10% of te data (aproximately)\n",
        "    index90 = [x for x in index if x not in index10] #makes a list with the remaining\n",
        "\n",
        "    vl_spect_df_comb = np_spect_comb[index10,]\n",
        "    tr_spect_df_comb = np_spect_comb[index90,]\n",
        "\n",
        "    vl_trait_df = np_trait[index10,]\n",
        "    tr_trait_df = np_trait[index90,]\n",
        "\n",
        "    #this is a function to re-scale inputs for the neural networks\n",
        "    scaler = MinMaxScaler()   \n",
        "\n",
        "    #spectral data:\n",
        "    X_train = tr_spect_df_comb\n",
        "    #Trait data:\n",
        "    Y_train = tr_trait_df\n",
        "    \n",
        "    #print(\"Training the gaussian processes @\" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    #gpr_ml_comb.fit(X_train,Y_train)\n",
        "    #print(\"Finished @\" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "    #preparing data for the multi-task network\n",
        "    scaler.fit(Y_train)\n",
        "    Y_train_norm = scaler.transform(Y_train)\n",
        "\n",
        "    cab_train = Y_train_norm[:,0]\n",
        "    cw_train = Y_train_norm[:,1]\n",
        "    cm_train = Y_train_norm[:,2]\n",
        "    lai_train = Y_train_norm[:,3]\n",
        "\n",
        "    print(\"Training the multi task neural network @ \" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    model_comb.fit(X_train,[cab_train,cw_train,cm_train,lai_train],epochs=number_epochs,verbose=0)#hides spam on the output\n",
        "    print(\"Finished @ \" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    #here i will put a flow to print the predictions on the out-of-bag samples later\n",
        "\n",
        "    #Fetching the data for the prediction\n",
        "    print(\"Fetched image: \"+'/content/drive/My Drive/Redouan_out/'+ out_s2_name + \".tif\")\n",
        "\n",
        "    #reads the raster\n",
        "    ds, bands = raster.read('/content/drive/My Drive/Redouan_out/'+ out_s2_name + \".tif\")\n",
        "    #ds holds the information to reconstruct the raster\n",
        "    #bands is the actual information\n",
        "    \n",
        "    #creates an array of  n(number of pixels) by 9 (bands of sentinel 2) -> also divides by 10000 to transform the bits to reflectance\n",
        "    bandByPixel = convert_rst(bands)\n",
        "\n",
        "    #Gaussian processes easily runs out of memory when doing predictions so we have to \"reduce the number of cells\"\n",
        "    #from sklearn.model_selection import KFold\n",
        "    #kf = KFold(n_splits=50)\n",
        "    #kf.split(bandByPixel)\n",
        "\n",
        "    ############## predicting the neural network, hopefully i don't run out of memory on this procedure\n",
        "    mtn_pred = model_comb.predict(bandByPixel)\n",
        "\n",
        "    #the output is a n(number of pixels) by 4 bands but the values have to be \"scaled back\"\n",
        "    cab_mtn = mtn_pred[0]\n",
        "    cw_mtn  = mtn_pred[1]\n",
        "    cm_mtn  = mtn_pred[2]\n",
        "    lai_mtn = mtn_pred[3]\n",
        "\n",
        "    np_Y_ann_pred = np.array(cab_mtn)\n",
        "    np_Y_ann_pred = np.hstack((np_Y_ann_pred,cw_mtn))\n",
        "    np_Y_ann_pred = np.hstack((np_Y_ann_pred,cm_mtn))\n",
        "    np_Y_ann_pred = np.hstack((np_Y_ann_pred,lai_mtn))\n",
        "    #inverting the min-max scaling\n",
        "\n",
        "    np_Y_ann_pred = scaler.inverse_transform(np_Y_ann_pred)\n",
        "\n",
        "    #and now we reconstruct the raster\n",
        "    mtn_cab_rst = np.reshape(np_Y_ann_pred[:,0],(ds.RasterYSize,ds.RasterXSize))\n",
        "    mtn_cw_rst = np.reshape(np_Y_ann_pred[:,1],(ds.RasterYSize,ds.RasterXSize))\n",
        "    mtn_cm_rst = np.reshape(np_Y_ann_pred[:,2],(ds.RasterYSize,ds.RasterXSize))\n",
        "    mtn_lai_rst = np.reshape(np_Y_ann_pred[:,3],(ds.RasterYSize,ds.RasterXSize))\n",
        "\n",
        "    #creating the filename for output\n",
        "    mtn_cab_name = '/content/drive/My Drive/Redouan_out/' + out_s2_name + \"_MTN_cab_05\"+\".tif\"\n",
        "    mtn_cw_name =  '/content/drive/My Drive/Redouan_out/' + out_s2_name + \"_MTN_cw_05\"+\".tif\"\n",
        "    mtn_cm_name =  '/content/drive/My Drive/Redouan_out/' + out_s2_name + \"_MTN_cm_05\"+\".tif\"\n",
        "    mtn_lai_name = '/content/drive/My Drive/Redouan_out/' + out_s2_name + \"_MTN_lai_05\"+\".tif\"\n",
        "\n",
        "    #saving the rasters\n",
        "    print(\"Saving the biophysical paramaters retrievals\")\n",
        "    raster.export(mtn_cab_rst, ds, mtn_cab_name, dtype='float')\n",
        "    raster.export(mtn_cw_rst, ds, mtn_cw_name, dtype='float')\n",
        "    raster.export(mtn_cm_rst, ds, mtn_cm_name, dtype='float')\n",
        "    raster.export(mtn_lai_rst, ds, mtn_lai_name, dtype='float')\n",
        "\n",
        "    k_set = k_set+1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDmty0712RI4"
      },
      "source": [
        "Auxiliar processing functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aX6XGia2V68"
      },
      "source": [
        "#Installing PROSAIL\n",
        "!pip install prosail\n",
        "\n",
        "#latin hypercube stuff\n",
        "#lets try to do a LHS\n",
        "!pip install lhsmdu\n",
        "\n",
        "#this package as a number of functions to deal with hyperspectral data\n",
        "!pip install pysptools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hALB94QV2Xws",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "8e27b500-9f9f-45bf-8bb2-1e9a9c6ce13c"
      },
      "source": [
        "!pip uninstall tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.2.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.2.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXMBMb-U3u7K"
      },
      "source": [
        "!pip install tensorflow==1.14.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrU9l8X37cD3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d381f18f-d624-4efa-e7d5-0947b2a7daa0"
      },
      "source": [
        "#importing stuff \n",
        "#General purpose: \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy import stats\n",
        "\n",
        "#the beutiful R like data frame\n",
        "import pandas as pd\n",
        "\n",
        "#PROSPECT+SAIL Radiative transfer mode package\n",
        "import prosail\n",
        "\n",
        "#Sampling design package\n",
        "import lhsmdu\n",
        "\n",
        "#a few more stuff for random\n",
        "import random as rdm\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "#package to for operations on spectral data\n",
        "import pysptools as sptool \n",
        "from pysptools import distance\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold # import KFold\n",
        "\n",
        "#machine learning stuff\n",
        "#NEURAL NETWORK - Keras will be updated soon so this colab will also have to be changed\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor as ANN_reg #this is a simpler neural network package\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import GaussianNoise\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#ignore the warning for now\n",
        "\n",
        "#Random FOREST\n",
        "# Fitting Random Forest Regression to the dataset \n",
        "# import the regressor \n",
        "from sklearn.ensemble import RandomForestRegressor \n",
        "\n",
        "#Gaussian processes\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel, Matern, WhiteKernel, ExpSineSquared, RationalQuadratic, RBF\n",
        "\n",
        "#aux functions\n",
        "from sklearn.preprocessing import MinMaxScaler #this is to standardize the input data [not used for now]\n",
        "from sklearn import metrics\n",
        "\n",
        "#for model storing -sklearn\n",
        "import pickle\n",
        "\n",
        "#for model storing keras\n",
        "from keras.models import model_from_json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZAFwciP7kK8"
      },
      "source": [
        "#curstom function to re-organize the raster into a structure i can use\n",
        "def convert_rst(bands):\n",
        "  bandByPixel = changeDimension(bands)/10000. #we have to devide all values by 10k - its a conversion from bits to reflectances\n",
        "  #bandByPixel_t = np.transpose(bandByPixel)\n",
        "  return bandByPixel\n",
        "\n",
        "\n",
        "#custom function to control prosail inputs\n",
        "def custom_prosail(cab,cw,cm,lai,tts,tto,phi):\n",
        "  import prosail\n",
        "  #default parameters\n",
        "  n= 1.\n",
        "  car=10.\n",
        "  cbrown=0.01\n",
        "  typelidf=1 #this is the default option\n",
        "  lidfa = -1 #leaf angle distribution parameter a and b\n",
        "  lidfb=-0.15\n",
        "  hspot= 0.01 #hotspot parameters - got this from R package https://www.rdocumentation.org/packages/hsdar/versions/0.4.1/topics/PROSAIL\n",
        "  #sun and viewing angle\n",
        "  #tts=30. #observation and solar position parameters\n",
        "  #tto=10. \n",
        "  #psi=0.\n",
        "  #for now i put them by hand but they should be an input of a custom function\n",
        "  tts=tts #solar zenith angle\n",
        "  tto=tto #sensor zenith angle\n",
        "  psi=phi\n",
        "  rho_out = prosail.run_prosail(n,\n",
        "                                 cab,\n",
        "                                 car,\n",
        "                                 cbrown,\n",
        "                                 cw,\n",
        "                                 cm,\n",
        "                                 lai,\n",
        "                                 lidfa,\n",
        "                                 hspot,\n",
        "                                 tts,tto,psi,\n",
        "                                 typelidf, lidfb,\n",
        "                                 prospect_version=\"D\",\n",
        "                                 factor='SDR', \n",
        "                                 rsoil=.5, psoil=.5)\n",
        "  return(rho_out)\n",
        "\n",
        "def Prosail2S2(path2csv,spectra_input):\n",
        "  #importing pandas\n",
        "  import pandas as pd\n",
        "  import numpy\n",
        "  import numpy as np\n",
        "  #upload a S2_Response.csv from https://earth.esa.int/web/sentinel/user-guides/sentinel-2-msi/document-library/-/asset_publisher/Wk0TKajiISaR/content/sentinel-2a-spectral-responses\n",
        "\n",
        "  s2_table = pd.read_csv(path2csv,sep=\";\",decimal=\",\") #check if this is proper, regarding the sep and dec\n",
        "  #chekc which row you are actually extracting\n",
        "\n",
        "  s2_table_sel = s2_table[s2_table['SR_WL'].between(400,2500)] #selects all values between 400 and 2500\n",
        "  spectra_input_df = pd.DataFrame(data=spectra_input,columns=[\"rho\"],index=s2_table_sel.index) #transforms the input array into a pandas df with the column name rho and row.index = to the original input table\n",
        "\n",
        "  \n",
        "  rho_s2 = s2_table_sel.multiply(spectra_input_df['rho'],axis=\"index\") #calculates the numerator\n",
        "  w_band_sum = s2_table_sel.sum(axis=0,skipna = True) #calculates the denominator\n",
        "\n",
        "  output = (rho_s2.sum(axis=0)/w_band_sum).rename_axis(\"ID\").values #runs the weighted mean and converts the output to a numpy array\n",
        "\n",
        "  return output[1:] #removes the first value because it represents the wavelength column\n",
        "\n",
        "#please LOAD THTE FILE NOW\n",
        "#filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "#filepath=\"/content/S2_Responses_S2B.csv\"\n",
        "#filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "\n",
        "\n",
        "def Gen_spectra_data(traits,tts,tto,phi):\n",
        "  k = 1\n",
        "  #pd_train_traits=traits\n",
        "  #print(range(len(traits)))\n",
        "  for i in range(len(traits)):\n",
        "    #n_t = pd_train_traits[\"n\"][i]\n",
        "    cab_t = traits[\"cab\"][i]\n",
        "    #car_t = pd_train_traits[\"car\"][i]\n",
        "    #cbrown_t = pd_train_traits[\"cbrown\"][i]\n",
        "    cw_t = traits[\"cw\"][i]\n",
        "    cm_t = traits[\"cm\"][i]\n",
        "    lai_t = traits[\"lai\"][i]\n",
        "\n",
        "    if k == 1:\n",
        "      tr_rho_s = custom_prosail(cab_t,cw_t,cm_t,lai_t,tts,tto,phi)\n",
        "      tr_rho_s = Prosail2S2(filepath,tr_rho_s)\n",
        "      #plt.plot ( x, tr_rho_s, ':', label=\"Training prosail\")\n",
        "      #plt.legend(loc='best')\n",
        "      \n",
        "    if k > 1:\n",
        "      tr_rho_t = custom_prosail(cab_t,cw_t,cm_t,lai_t,tts,tto,phi)\n",
        "      tr_rho_t = Prosail2S2(filepath,tr_rho_t)\n",
        "      tr_rho_s = np.vstack((tr_rho_s,tr_rho_t))\n",
        "      #plt.plot ( x, tr_rho_t, ':')\n",
        "\n",
        "    k = k+1\n",
        "\n",
        "\n",
        "  rho_samples=tr_rho_s\n",
        "\n",
        "\n",
        "  return rho_samples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Function to store the outputs into a table\n",
        "column_names=[\"Model\",\n",
        "              \"NoiseLevel\",\n",
        "              \"NoiseType\",\n",
        "              \"OutOfBag\",\n",
        "              \"KFold_tr_samples\",\n",
        "              \"KFold_vl_samples\",\n",
        "              \"Variable\",\n",
        "              \"Fold_nr\",\n",
        "              \"ExplVar\",\n",
        "              \"Max_err\",\n",
        "              \"Mean_abs_Err\",\n",
        "              \"Mean_sqr_err\",\n",
        "              #\"Mean_sqr_lg_err\",\n",
        "              \"Median_abs_err\",\n",
        "              \"r2\",\n",
        "              \"MAPE\",\n",
        "              \"ModelName\"]\n",
        "              #\"Mean_poiss_dev\",\n",
        "              #\"Mean_gamma_dev\"]\n",
        "              #\"Mean_tweed_dev\"]\n",
        "\n",
        "#mape is not existant in the package so we have to create it:\n",
        "#https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn\n",
        "#from sklearn.utils import check_array\n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "\n",
        "    ## Note: does not handle mix 1d representation\n",
        "    #if _is_1d(y_true): \n",
        "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
        "\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "  #Here, the file is used for saving\n",
        "  #creating a df to receive the data\n",
        "\n",
        "\n",
        "def calc_metrics(MDL,NoiseLevel,NoiseType,oob_samples,kf_tr,kf_vl,Variable,Fold,Ref,Pred,Modelname):\n",
        "\n",
        "  out_list = {\"Model\":MDL,\n",
        "              \"NoiseLevel\":NoiseLevel,\n",
        "              \"NoiseType\":NoiseType,\n",
        "              \"OutOfBag\":oob_samples,\n",
        "              \"KFold_tr_samples\":kf_tr,\n",
        "              \"KFold_vl_samples\":kf_vl,\n",
        "              \"Variable\":Variable,\n",
        "              \"Fold_nr\":Fold,\n",
        "              \"ExplVar\": metrics.explained_variance_score(Ref, Pred),\n",
        "              \"Max_err\": metrics.max_error(Ref, Pred),\n",
        "              \"Mean_abs_Err\": metrics.mean_absolute_error(Ref, Pred),\n",
        "              \"Mean_sqr_err\": metrics.mean_squared_error(Ref, Pred),\n",
        "              \"Median_abs_err\" : metrics.median_absolute_error(Ref, Pred),\n",
        "              \"r2\": metrics.r2_score(Ref, Pred),\n",
        "              \"MAPE\": mean_absolute_percentage_error(Ref, Pred),\n",
        "              \"ModelName\":Modelname}\n",
        "\n",
        "\n",
        "  return out_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lizKQb07t5t"
      },
      "source": [
        "def combined_noise(ref,sigma):\n",
        "\n",
        "  #ref is an input vector and sigma is the standar deviation of the noise\n",
        "\n",
        "  #this format implies some values go negative or above 1 which is terrible\n",
        "\n",
        "  #the multiplier parameter is solid\n",
        "  #mp_noise = ref*(1 + np.random.normal(0,scale=2*sigma,size=ref.shape[0]))\n",
        "\n",
        "  #this form makes the value go over 1 and under 0 from time to time\n",
        "  #ad_noise = 0#+np.random.normal(0,sigma)\n",
        "\n",
        "  #truncated version - the limits are on 2*sigma to give it a leeway \n",
        "  #ad_noise = 0+stats.truncnorm.rvs(0-sigma*2,0+sigma*2,loc=0,scale=sigma,size=ref.shape[0])\n",
        "\n",
        "  #the only way i can think to force it doesn't go over 1 or under 0, is to use a naive verifier\n",
        "  \n",
        "  #sigma is any value between 0 and 1, reflecting \"% percent error\"\n",
        "  #notice i give leeway to sigma by doubling the interval but the truncated version should be stronger againt bad values\n",
        "  \n",
        "  mp_noise = ref*(1 + stats.truncnorm.rvs(0-sigma*4,0+sigma*4,loc=0,scale=sigma*2,size=ref.shape[0]))\n",
        "  ad_noise = 0+stats.truncnorm.rvs(0-sigma*2,0+sigma*2,loc=0,scale=sigma,size=ref.shape[0])\n",
        "  out_ref = mp_noise+ ad_noise\n",
        "\n",
        "\n",
        "  #this looks fun but breaks the speed of processing....\n",
        "  # while max(out_ref) > 1 or min(out_ref)<0:\n",
        "  #   #print(\"i am in the loop!\")\n",
        "  #   mp_noise = ref*(1 + stats.truncnorm.rvs(0-sigma*4,0+sigma*4,loc=0,scale=sigma*2,size=ref.shape[0]))\n",
        "  #   ad_noise = 0+stats.truncnorm.rvs(0-sigma*2,0+sigma*2,loc=0,scale=sigma,size=ref.shape[0])\n",
        "  #   out_ref = mp_noise+ ad_noise\n",
        "  #   #print(out_ref)\n",
        "\n",
        "  #making everything that goes under, be 0 or 1\n",
        "  out_ref[out_ref>1]=1\n",
        "  out_ref[out_ref<0]=0\n",
        "\n",
        "  return out_ref\n",
        "\n",
        "def inverted_noise(ref,sigma):\n",
        "\n",
        "  #ref is an input vector and sigma is the standar deviation of the noise\n",
        "\n",
        "\n",
        "  #in this case, both of these are producing improper results - less than 0 or more than 1 oftne. \n",
        "  #mp_noise = 1-(abs(1-ref)*(1 + np.random.normal(0,scale=2*sigma,size=ref.shape[0])))\n",
        "  #ad_noise = 0#+np.random.normal(0,sigma)\n",
        "\n",
        "  mp_noise = 1-(1-ref)*(1 + stats.truncnorm.rvs(0-sigma*4,0+sigma*4,loc=0,scale=sigma*2,size=ref.shape[0]))\n",
        "  ad_noise = 0+stats.truncnorm.rvs(0-sigma*2,0+sigma*2,loc=0,scale=sigma,size=ref.shape[0])\n",
        "\n",
        "  out_ref = mp_noise+ ad_noise\n",
        "\n",
        "  #this looks fun but breaks the speed of processing...\n",
        "  # while max(out_ref) > 1 or min(out_ref)<0:\n",
        "  #   #print(\"i am in the loop!\")\n",
        "  #   mp_noise = 1-(1-ref)*(1 + stats.truncnorm.rvs(0-sigma*4,0+sigma*4,loc=0,scale=sigma*2,size=ref.shape[0]))\n",
        "  #   ad_noise = 0+stats.truncnorm.rvs(0-sigma*2,0+sigma*2,loc=0,scale=sigma,size=ref.shape[0])\n",
        "  #   out_ref = mp_noise+ ad_noise\n",
        "  #   #print(out_ref)  while \n",
        "\n",
        "\n",
        "  #making everything that goes under, be 0 or 1\n",
        "  out_ref[out_ref>1]=1\n",
        "  out_ref[out_ref<0]=0\n",
        "\n",
        "  return out_ref\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LYAGvpz7mYN"
      },
      "source": [
        "pr_ml_comb = GaussianProcessRegressor(n_restarts_optimizer=25, # -> 1*RBF(1.0) is the default kernel\n",
        "                                       normalize_y=True,\n",
        "                                       random_state=0,\n",
        "                                       alpha=.05) #this is for noise\n",
        "\n",
        "gpr_ml_inve = GaussianProcessRegressor(n_restarts_optimizer=25, # -> 1*RBF(1.0) is the default kernel\n",
        "                                       normalize_y=True,\n",
        "                                       random_state=0)\n",
        "\n",
        "#multi-task neural network - 16 x 8 shared x 4 individual\n",
        "inputs = Input(shape=(9,))\n",
        "#noise = GaussianNoise(.05)(inputs) #im not sure this is how we use this function\n",
        "sub1 = Dense(16, activation='tanh')(inputs)\n",
        "sub2 = Dense(8, activation='tanh')(sub1)\n",
        "cab1 = Dense(4, activation='sigmoid')(sub2)\n",
        "cw1  = Dense(4, activation='sigmoid')(sub2)\n",
        "cm1  = Dense(4, activation='sigmoid')(sub2)\n",
        "lai1 = Dense(4, activation='sigmoid')(sub2)\n",
        "cab2 = Dense(1, activation='linear')(cab1)\n",
        "cw2  = Dense(1, activation='linear')(cw1)\n",
        "cm2  = Dense(1, activation='linear')(cm1)\n",
        "lai2 = Dense(1, activation='linear')(lai1)\n",
        "\n",
        "\n",
        "#building the model using keras api\n",
        "model_comb = Model(inputs=inputs, outputs=[cab2,cw2,cm2,lai2])\n",
        "model_inve = Model(inputs=inputs, outputs=[cab2,cw2,cm2,lai2])\n",
        "\n",
        "model_comb.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "model_inve.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#metas:\n",
        "number_epochs = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6guAHNor7pcs"
      },
      "source": [
        "n_traits=4 #I will test on 4 varying traits: cab, car, cw,cm,lai\n",
        "\n",
        "#generating a LHS hypercube (it uses a 0 to 1 interval that can be used as a multiplier against the different traits)\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "#the values here are 1 less than the max so i can add a value later to it\n",
        "#max_n=1 \n",
        "max_cab=121. #add 1\n",
        "#max_car=44. #add 1\n",
        "#max_cbrown= 9.99 #add 0.01\n",
        "max_cw=0.008 #add 0.001 \n",
        "max_cm=0.008 #0.001\n",
        "max_lai = 9.9 #add 0.1\n",
        "\n",
        "min_cab = 10.\n",
        "min_cw = 0.002\n",
        "min_cm = 0.002\n",
        "min_lai = .5\n",
        "\n",
        "#labelling\n",
        "#label_names = [\"cab\",\"cw\",\"cm\",\"lai\"]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}